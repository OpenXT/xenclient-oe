################################################################################
SHORT DESCRIPTION:
################################################################################
Expose vcpus as multiple cores in a smaller number of sockets,
by adjusting the cpuid responses appropriately.

################################################################################
LONG DESCRIPTION:
################################################################################
Exposes VCPUs as multiple cores in a smaller number of sockets, by adjusting the
CPUID responses appropriately.

This enables presentation of cores rather than CPUs to guest VM operating
systems which may implement CPU count limits or licensing constraints.

The introduced XEN_DOMCTL_setcorespersocket op configures Xen to deal with the
differences between cores and CPUs.

The separate hide-cores-from-cpuid patch assists with masking the effects of
core changes from guests.

################################################################################
CHANGELOG
################################################################################
March 2017:
Retrieved from XenServer's 4.7 patch queue to replace the many-patches-in-one
"hvm-cpuid-multicore" from earlier OpenXT / XenClient, and modified to apply to
Xen 4.8.1.

Further modified to apply to Xen 4.9 as upstream CPUID code has changed.

################################################################################
REMOVAL
################################################################################
No.

################################################################################
UPSTREAM PLAN
################################################################################
Unknown. XenServer still carries a version of this in their 4.7 patch queue
and the comments inline in the patch describe unpleasantness versus the AMD
platform specification.

################################################################################
INTERNAL DEPENDENCIES
################################################################################
See hide-cores-from-cpuid patch

################################################################################
PATCHES
################################################################################
--- a/tools/libxc/include/xenctrl.h
+++ b/tools/libxc/include/xenctrl.h
@@ -1336,6 +1336,10 @@ int xc_domain_get_tsc_info(xc_interface
 
 int xc_domain_disable_migrate(xc_interface *xch, uint32_t domid);
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket);
+
 int xc_domain_maximum_gpfn(xc_interface *xch, domid_t domid, xen_pfn_t *gpfns);
 
 int xc_domain_nr_gpfns(xc_interface *xch, domid_t domid, xen_pfn_t *gpfns);
--- a/tools/libxc/xc_domain.c
+++ b/tools/libxc/xc_domain.c
@@ -874,6 +874,20 @@ int xc_domain_get_tsc_info(xc_interface
 }
 
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket)
+{
+    struct xen_domctl domctl =
+    {
+        .cmd = XEN_DOMCTL_setcorespersocket,
+        .domain = domid,
+        .u.corespersocket.cores_per_socket = cores_per_socket,
+    };
+
+    return do_domctl(xch, &domctl);
+}
+
 int xc_domain_maximum_gpfn(xc_interface *xch, domid_t domid, xen_pfn_t *gpfns)
 {
     long rc = do_memory_op(xch, XENMEM_maximum_gpfn, &domid, sizeof(domid));
--- a/xen/arch/x86/cpuid.c
+++ b/xen/arch/x86/cpuid.c
@@ -681,6 +681,7 @@ void guest_cpuid(const struct vcpu *v, u
 {
     const struct domain *d = v->domain;
     const struct cpuid_policy *p = d->arch.cpuid;
+    const unsigned int cores_per_socket = v->domain->cores_per_socket;
 
     *res = EMPTY_LEAF;
 
@@ -706,6 +707,14 @@ void guest_cpuid(const struct vcpu *v, u
                 return;
 
             *res = p->cache.raw[subleaf];
+
+            if ( p->x86_vendor == X86_VENDOR_INTEL &&
+                 cores_per_socket > 0 )
+            {
+                /* fake out cores per socket */
+                res->a &= 0x3fff; /* one thread, one core */
+                res->a |= (((cores_per_socket * 2) - 1) << 26);
+            }
             break;
 
         case 0x7:
@@ -776,10 +785,31 @@ void guest_cpuid(const struct vcpu *v, u
         const struct cpu_user_regs *regs;
 
     case 0x1:
-        /* TODO: Rework topology logic. */
-        res->b &= 0x00ffffffu;
-        if ( is_hvm_domain(d) )
-            res->b |= (v->vcpu_id * 2) << 24;
+        if ( cores_per_socket > 0 )
+        {
+            /* to fake out #vcpus per socket first force on HT/MC */
+            res->d |= cpufeat_mask(X86_FEATURE_HTT);
+            /* fake out #vcpus and inform guest of #cores per package */
+            res->b &= 0xff00ffff;
+            /*
+             * For AMD, this (cores_per_socket * 2) is wrong, and contrary
+             * to the statement in the AMD manual.
+             * However, Xen unconditionally offers Intel-style APIC IDs
+             * (odd IDs for hyperthreads) which breaks the AMD APIC
+             * Enumeration Requirements.
+             *
+             * Fake up cores-per-socket as a socket with twice as many cores
+             * as expected, with every odd core offline.
+             */
+            res->b |= (((cores_per_socket * 2) & 0xff) << 16);
+        }
+        else
+        {
+            /* TODO: Rework topology logic. */
+            res->b &= 0x00ffffffu;
+            if ( is_hvm_domain(d) )
+                res->b |= (v->vcpu_id * 2) << 24;
+        }
 
         /* TODO: Rework vPMU control in terms of toolstack choices. */
         if ( vpmu_available(v) &&
@@ -1035,6 +1065,25 @@ void guest_cpuid(const struct vcpu *v, u
         }
         break;
 
+    case 0x80000008:
+        if ( p->x86_vendor == X86_VENDOR_AMD &&
+            cores_per_socket > 0 )
+        {
+            res->c &= 0xffff0f00;
+            /*
+             * This (cores_per_socket * 2) is wrong, and contrary to the statement
+             * in the AMD manual.
+             * However, Xen unconditionally offers Intel-style APIC IDs
+             * (odd IDs for hyperthreads) which breaks the AMD APIC
+             * Enumeration Requirements.
+             *
+             * Fake up cores-per-socket as a socket with twice as many cores
+             * as expected, with every odd core offline.
+             */
+            res->c |= ((cores_per_socket * 2) - 1) & 0xff;
+        }
+        break;
+
     case 0x8000001c:
         if ( (v->arch.xcr0 & XSTATE_LWP) && cpu_has_svm )
             /* Turn on available bit and other features specified in lwp_cfg. */
--- a/xen/common/domctl.c
+++ b/xen/common/domctl.c
@@ -1142,6 +1142,32 @@ long do_domctl(XEN_GUEST_HANDLE_PARAM(xe
             copyback = 1;
         break;
 
+    case XEN_DOMCTL_setcorespersocket:
+    {
+        unsigned int cps = op->u.corespersocket.cores_per_socket;
+
+        /* Toolstack is permitted to set this value exactly once. */
+        if ( d->cores_per_socket != 0 )
+            ret = -EEXIST;
+
+        /* Only meaningful for HVM domains. */
+        else if ( !is_hvm_domain(d) )
+            ret = -EOPNOTSUPP;
+
+        /* Cores per socket is strictly within the bounds of max_vcpus. */
+        else if ( cps < 1 || cps > d->max_vcpus )
+            ret = -EINVAL;
+
+        /* Cores per socket must exactly divide max_vcpus. */
+        else if ( d->max_vcpus % cps != 0 )
+            ret = -EDOM;
+
+        else
+            d->cores_per_socket = cps;
+
+        break;
+    }
+
     default:
         ret = arch_do_domctl(op, d, u_domctl);
         break;
--- a/xen/include/public/domctl.h
+++ b/xen/include/public/domctl.h
@@ -1141,6 +1141,13 @@ struct xen_domctl_psr_cat_op {
 typedef struct xen_domctl_psr_cat_op xen_domctl_psr_cat_op_t;
 DEFINE_XEN_GUEST_HANDLE(xen_domctl_psr_cat_op_t);
 
+struct xen_domctl_corespersocket {
+    uint32_t cores_per_socket;
+};
+
+typedef struct xen_domctl_corespersocket xen_domctl_corespersocket_t;
+DEFINE_XEN_GUEST_HANDLE(xen_domctl_corespersocket_t);
+
 struct xen_domctl {
     uint32_t cmd;
 #define XEN_DOMCTL_createdomain                   1
@@ -1222,6 +1229,7 @@ struct xen_domctl {
 #define XEN_DOMCTL_gdbsx_pausevcpu             1001
 #define XEN_DOMCTL_gdbsx_unpausevcpu           1002
 #define XEN_DOMCTL_gdbsx_domstatus             1003
+#define XEN_DOMCTL_setcorespersocket           4001
     uint32_t interface_version; /* XEN_DOMCTL_INTERFACE_VERSION */
     domid_t  domain;
     union {
@@ -1271,6 +1279,7 @@ struct xen_domctl {
         struct xen_domctl_audit_p2m         audit_p2m;
         struct xen_domctl_set_virq_handler  set_virq_handler;
         struct xen_domctl_set_max_evtchn    set_max_evtchn;
+        struct xen_domctl_corespersocket    corespersocket;
         struct xen_domctl_gdbsx_memio       gdbsx_guest_memio;
         struct xen_domctl_set_broken_page_p2m set_broken_page_p2m;
         struct xen_domctl_cacheflush        cacheflush;
--- a/xen/include/xen/sched.h
+++ b/xen/include/xen/sched.h
@@ -484,6 +484,8 @@ struct domain
         unsigned int guest_request_enabled       : 1;
         unsigned int guest_request_sync          : 1;
     } monitor;
+
+    unsigned int cores_per_socket;
 };
 
 /* Protect updates/reads (resp.) of domain_list and domain_hash. */
--- a/xen/xsm/flask/hooks.c
+++ b/xen/xsm/flask/hooks.c
@@ -748,6 +748,9 @@ static int flask_domctl(struct domain *d
     case XEN_DOMCTL_soft_reset:
         return current_has_perm(d, SECCLASS_DOMAIN2, DOMAIN2__SOFT_RESET);
 
+    case XEN_DOMCTL_setcorespersocket:
+        return current_has_perm(d, SECCLASS_DOMAIN2, DOMAIN2__SETCORESPERSOCKET);
+
     default:
         return avc_unknown_permission("domctl", cmd);
     }
--- a/xen/xsm/flask/policy/access_vectors
+++ b/xen/xsm/flask/policy/access_vectors
@@ -246,6 +246,8 @@ class domain2
     mem_sharing
 # XEN_DOMCTL_psr_cat_op
     psr_cat_op
+# XEN_DOMCTL_setcorespersocket
+    setcorespersocket
 }
 
 # Similar to class domain, but primarily contains domctls related to HVM domains
