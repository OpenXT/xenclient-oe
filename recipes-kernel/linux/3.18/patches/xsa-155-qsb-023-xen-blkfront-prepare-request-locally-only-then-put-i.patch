################################################################################
SHORT DESCRIPTION: 
################################################################################
QSB-023 (https://github.com/QubesOS/qubes-secpack/blob/master/QSBs/qsb-023-2015)
XSA-155 additional patches for the Xen network and block frontends from Qubes
OS Project.

################################################################################
LONG DESCRIPTION: 
################################################################################
Source: https://github.com/QubesOS/qubes-linux-kernel/tree/stable-3.18/patches.xen
Patch: xsa155-linux318-0013-xen-blkfront-prepare-request-locally-only-then-put-i.patch 

Do not reuse data which theoretically might be already modified by the
backend. This is mostly about private copy of the request
(info->shadow[id].req) - make sure the request saved there is really the
one just filled.

################################################################################
PATCHES 
################################################################################
Index: linux-3.18.25/drivers/block/xen-blkfront.c
===================================================================
--- linux-3.18.25.orig/drivers/block/xen-blkfront.c	2015-12-22 15:09:48.830291194 +0100
+++ linux-3.18.25/drivers/block/xen-blkfront.c	2015-12-22 15:11:37.349169528 +0100
@@ -389,7 +389,7 @@
 static int blkif_queue_request(struct request *req)
 {
 	struct blkfront_info *info = req->rq_disk->private_data;
-	struct blkif_request *ring_req;
+	struct blkif_request ring_req;
 	unsigned long id;
 	unsigned int fsect, lsect;
 	int i, ref, n;
@@ -434,42 +434,42 @@
 		new_persistent_gnts = 0;
 
 	/* Fill out a communications ring structure. */
-	ring_req = RING_GET_REQUEST(&info->ring, info->ring.req_prod_pvt);
+	RING_COPY_REQUEST(&info->ring, info->ring.req_prod_pvt, &ring_req);
 	id = get_id_from_freelist(info);
 	info->shadow[id].request = req;
 
 	if (unlikely(req->cmd_flags & (REQ_DISCARD | REQ_SECURE))) {
-		ring_req->operation = BLKIF_OP_DISCARD;
-		ring_req->u.discard.nr_sectors = blk_rq_sectors(req);
-		ring_req->u.discard.id = id;
-		ring_req->u.discard.sector_number = (blkif_sector_t)blk_rq_pos(req);
+		ring_req.operation = BLKIF_OP_DISCARD;
+		ring_req.u.discard.nr_sectors = blk_rq_sectors(req);
+		ring_req.u.discard.id = id;
+		ring_req.u.discard.sector_number = (blkif_sector_t)blk_rq_pos(req);
 		if ((req->cmd_flags & REQ_SECURE) && info->feature_secdiscard)
-			ring_req->u.discard.flag = BLKIF_DISCARD_SECURE;
+			ring_req.u.discard.flag = BLKIF_DISCARD_SECURE;
 		else
-			ring_req->u.discard.flag = 0;
+			ring_req.u.discard.flag = 0;
 	} else {
 		BUG_ON(info->max_indirect_segments == 0 &&
 		       req->nr_phys_segments > BLKIF_MAX_SEGMENTS_PER_REQUEST);
 		BUG_ON(info->max_indirect_segments &&
 		       req->nr_phys_segments > info->max_indirect_segments);
 		nseg = blk_rq_map_sg(req->q, req, info->shadow[id].sg);
-		ring_req->u.rw.id = id;
+		ring_req.u.rw.id = id;
 		if (nseg > BLKIF_MAX_SEGMENTS_PER_REQUEST) {
 			/*
 			 * The indirect operation can only be a BLKIF_OP_READ or
 			 * BLKIF_OP_WRITE
 			 */
 			BUG_ON(req->cmd_flags & (REQ_FLUSH | REQ_FUA));
-			ring_req->operation = BLKIF_OP_INDIRECT;
-			ring_req->u.indirect.indirect_op = rq_data_dir(req) ?
+			ring_req.operation = BLKIF_OP_INDIRECT;
+			ring_req.u.indirect.indirect_op = rq_data_dir(req) ?
 				BLKIF_OP_WRITE : BLKIF_OP_READ;
-			ring_req->u.indirect.sector_number = (blkif_sector_t)blk_rq_pos(req);
-			ring_req->u.indirect.handle = info->handle;
-			ring_req->u.indirect.nr_segments = nseg;
+			ring_req.u.indirect.sector_number = (blkif_sector_t)blk_rq_pos(req);
+			ring_req.u.indirect.handle = info->handle;
+			ring_req.u.indirect.nr_segments = nseg;
 		} else {
-			ring_req->u.rw.sector_number = (blkif_sector_t)blk_rq_pos(req);
-			ring_req->u.rw.handle = info->handle;
-			ring_req->operation = rq_data_dir(req) ?
+			ring_req.u.rw.sector_number = (blkif_sector_t)blk_rq_pos(req);
+			ring_req.u.rw.handle = info->handle;
+			ring_req.operation = rq_data_dir(req) ?
 				BLKIF_OP_WRITE : BLKIF_OP_READ;
 			if (req->cmd_flags & (REQ_FLUSH | REQ_FUA)) {
 				/*
@@ -479,15 +479,15 @@
 				 * way.  (It's also a FLUSH+FUA, since it is
 				 * guaranteed ordered WRT previous writes.)
 				 */
-				ring_req->operation = info->flush_op;
+				ring_req.operation = info->flush_op;
 			}
-			ring_req->u.rw.nr_segments = nseg;
+			ring_req.u.rw.nr_segments = nseg;
 		}
 		for_each_sg(info->shadow[id].sg, sg, nseg, i) {
 			fsect = sg->offset >> 9;
 			lsect = fsect + (sg->length >> 9) - 1;
 
-			if ((ring_req->operation == BLKIF_OP_INDIRECT) &&
+			if ((ring_req.operation == BLKIF_OP_INDIRECT) &&
 			    (i % SEGS_PER_INDIRECT_FRAME == 0)) {
 				unsigned long uninitialized_var(pfn);
 
@@ -508,7 +508,7 @@
 				gnt_list_entry = get_grant(&gref_head, pfn, info);
 				info->shadow[id].indirect_grants[n] = gnt_list_entry;
 				segments = kmap_atomic(pfn_to_page(gnt_list_entry->pfn));
-				ring_req->u.indirect.indirect_grefs[n] = gnt_list_entry->gref;
+				ring_req.u.indirect.indirect_grefs[n] = gnt_list_entry->gref;
 			}
 
 			gnt_list_entry = get_grant(&gref_head, page_to_pfn(sg_page(sg)), info);
@@ -541,8 +541,8 @@
 				kunmap_atomic(bvec_data);
 				kunmap_atomic(shared_data);
 			}
-			if (ring_req->operation != BLKIF_OP_INDIRECT) {
-				ring_req->u.rw.seg[i] =
+			if (ring_req.operation != BLKIF_OP_INDIRECT) {
+				ring_req.u.rw.seg[i] =
 						(struct blkif_request_segment) {
 							.gref       = ref,
 							.first_sect = fsect,
@@ -560,10 +560,13 @@
 			kunmap_atomic(segments);
 	}
 
+	/* make the request available to the backend */
+	*RING_GET_REQUEST(&info->ring, info->ring.req_prod_pvt) = ring_req;
+	wmb();
 	info->ring.req_prod_pvt++;
 
 	/* Keep a private copy so we can reissue requests when recovering. */
-	info->shadow[id].req = *ring_req;
+	info->shadow[id].req = ring_req;
 
 	if (new_persistent_gnts)
 		gnttab_free_grant_references(gref_head);
