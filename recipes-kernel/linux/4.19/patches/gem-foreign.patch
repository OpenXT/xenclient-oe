--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -71,6 +71,7 @@ i915-y += i915_cmd_parser.o \
 	  i915_gem_shrinker.o \
 	  i915_gem_stolen.o \
 	  i915_gem_tiling.o \
+	  i915_gem_foreign.o \
 	  i915_gem_userptr.o \
 	  i915_gemfs.o \
 	  i915_query.o \
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -3295,6 +3295,8 @@ unsigned long i915_gem_shrink(struct drm
 			      unsigned long target,
 			      unsigned long *nr_scanned,
 			      unsigned flags);
+int i915_gem_foreign_ioctl(struct drm_device *dev, void *data,
+			   struct drm_file *file);
 #define I915_SHRINK_PURGEABLE 0x1
 #define I915_SHRINK_UNBOUND 0x2
 #define I915_SHRINK_BOUND 0x4
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_gem_foreign.c
@@ -0,0 +1,423 @@
+/*
+ * Copyright Â© 2013 Citrix Systems, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+
+#include <drm/drmP.h>
+#include <drm/i915_drm.h>
+#include "i915_drv.h"
+#include "i915_trace.h"
+#include "intel_drv.h"
+#include <linux/mmu_notifier.h>
+#include <linux/swap.h>
+#include <linux/gfp.h>
+#include <asm/xen/hypervisor.h>
+#include <asm/xen/hypercall.h>
+#include <xen/page.h>
+#include <xen/xen-ops.h>
+#include <xen/balloon.h>
+#include <xen/interface/memory.h>
+
+static inline void
+free_pages_hm(struct page **pages, size_t n)
+{
+	size_t i;
+
+	for (i = 0; i < n; ++i)
+		__free_pages(pages[i], 0);
+}
+
+static inline int
+alloc_pages_hm(struct page **pages, size_t n)
+{
+	size_t i;
+
+	for (i = 0; i < n; ++i) {
+		pages[i] = alloc_page(GFP_HIGHUSER);
+		if (!pages[i]) {
+			DRM_DEBUG("alloc_page() failed.\n");
+			free_pages_hm(pages, i - 1);
+			return -ENOMEM;
+		}
+	}
+	return 0;
+}
+
+static inline void
+free_pages_xb(struct page **pages, size_t n)
+{
+	free_xenballooned_pages(n, pages);
+}
+
+static inline int
+alloc_pages_xb(struct page **pages, size_t n)
+{
+	return alloc_xenballooned_pages(n, pages);
+}
+
+static int
+foreign_set_p2m(unsigned long mfn, struct page *page, unsigned long *omfn)
+{
+	unsigned long pfn;
+	unsigned long uninitialized_var(address);
+	unsigned int level;
+	pte_t *ptep;
+
+	pfn = page_to_pfn(page);
+	if (!PageHighMem(page)) {
+		/* Check this page is backed by something. */
+		address = (unsigned long)__va(pfn << PAGE_SHIFT);
+		ptep = lookup_address(address, &level);
+		if (!ptep || level != PG_LEVEL_4K) {
+			DRM_ERROR("%s: pfn %lx is not mapped.\n", __func__, pfn);
+			return -EINVAL;
+		}
+	}
+	if (likely(omfn != NULL))
+		/* Store the original mfn index, for reset later. */
+		*omfn = pfn_to_mfn(pfn);
+
+	/* Set the p2m. */
+	if (unlikely(!set_phys_to_machine(pfn, FOREIGN_FRAME(mfn)))) {
+		DRM_DEBUG("set_phys_to_machine(%#lx, %#lx) failed.\n", pfn, mfn);
+		*omfn = 0UL;
+		return -ENOMEM;
+        }
+
+	return 0;
+}
+
+static int
+foreign_reset_p2m(struct page *page, unsigned long omfn)
+{
+	unsigned long pfn;
+	unsigned long mfn;
+	unsigned long uninitialized_var(address);
+	unsigned int level;
+	pte_t *ptep;
+
+	pfn = page_to_pfn(page);
+	mfn = get_phys_to_machine(pfn);
+	/* Check we did put that page there in the first place. */
+	if (mfn == INVALID_P2M_ENTRY || !(mfn & FOREIGN_FRAME_BIT)) {
+		DRM_ERROR("%s: pfn %lx is not in the p2m.\n", __func__, pfn);
+		return -EINVAL;
+	}
+	if (!PageHighMem(page)) {
+		/* Check this page is backed by something. */
+		address = (unsigned long)__va(pfn << PAGE_SHIFT);
+		ptep = lookup_address(address, &level);
+		if (!ptep || level != PG_LEVEL_4K) {
+			DRM_ERROR("%s: pfn %lx is not mapped.\n",
+				  __func__, pfn);
+			return -EINVAL;
+		}
+	}
+	/* Revert to the original backing mfn index. */
+	set_phys_to_machine(pfn, omfn);
+	return 0;
+}
+
+
+
+static inline int
+i915_gem_foreign_alloc_pages(struct i915_gem_foreign *fo)
+{
+	int rc = 0;
+
+	switch (fo->flags) {
+		case 0:
+			rc = alloc_pages_hm(fo->pvec, fo->num_pages);
+			break;
+		case I915_FOREIGN_BALLOON_PAGES:
+			rc = alloc_pages_xb(fo->pvec, fo->num_pages);
+			break;
+		default:
+			DRM_ERROR("Unknown flag %#x.\n", fo->flags);
+			/* Rollback is done in alloc_pages helpers. */
+			return -EINVAL;
+	}
+	return rc;
+}
+
+static inline void
+i915_gem_foreign_free_pages(struct i915_gem_foreign *fo)
+{
+	switch (fo->flags) {
+		case 0:
+			free_pages_hm(fo->pvec, fo->num_pages);
+			break;
+		case I915_FOREIGN_BALLOON_PAGES:
+			free_pages_xb(fo->pvec, fo->num_pages);
+			break;
+		default:
+			DRM_ERROR("Unknown flag %#x.\n", fo->flags);
+	}
+}
+
+static inline void
+__i915_gem_foreign_reset_p2m(struct i915_gem_foreign *fo, size_t n)
+{
+	size_t i;
+
+	for (i = 0; i < n; ++i)
+		foreign_reset_p2m(fo->pvec[i], fo->mfns_ovr[i]);
+}
+
+static int
+i915_gem_foreign_set_p2m(struct i915_gem_foreign *fo)
+{
+	size_t i;
+	int ret;
+
+	for (i = 0; i < fo->num_pages; ++i) {
+		ret = foreign_set_p2m(fo->mfns[i], fo->pvec[i],
+				     &(fo->mfns_ovr[i]));
+		if (ret) {
+			__i915_gem_foreign_reset_p2m(fo, i - 1);
+			return ret;
+		}
+	}
+	return 0;
+}
+
+static void
+i915_gem_foreign_reset_p2m(struct i915_gem_foreign *fo)
+{
+	__i915_gem_foreign_reset_p2m(fo, fo->num_pages);
+}
+
+
+#if IS_ENABLED(CONFIG_SWIOTLB)
+# define swiotlb_active() swiotlb_nr_tbl()
+#else
+# define swiotlb_active() 0
+#endif
+
+static int
+st_set_pages(struct sg_table **st, struct page **pvec, int num_pages)
+{
+	struct scatterlist *sg;
+	int ret, n;
+
+	*st = kmalloc(sizeof(**st), GFP_KERNEL);
+	if (*st == NULL)
+		return -ENOMEM;
+
+	if (!swiotlb_active()) {
+		DRM_ERROR("No swiotlb detected."
+			  "i915_gem_foreign is Xen specific,"
+			  "which should enforce swiotlb.\n");
+		return -ENOENT;
+	}
+
+	ret = sg_alloc_table(*st, num_pages, GFP_KERNEL);
+	if (ret)
+		goto err;
+
+	for_each_sg((*st)->sgl, sg, num_pages, n)
+		sg_set_page(sg, pvec[n], PAGE_SIZE, 0);
+
+	return 0;
+
+err:
+	kfree(*st);
+	*st = NULL;
+	return ret;
+}
+
+static int 
+i915_gem_foreign_get_pages(struct drm_i915_gem_object *obj)
+{
+	struct i915_gem_foreign *fo = &obj->foreign;
+	int ret;
+	unsigned int sg_page_sizes;
+
+	fo->pvec = kvmalloc_array(fo->num_pages, sizeof (struct page *), GFP_KERNEL);
+	if (!fo->pvec)
+		return -ENOMEM;
+
+	ret = i915_gem_foreign_alloc_pages(fo);
+	if (ret) {
+		DRM_ERROR("Failed to allocate pages.\n");
+		goto fail_pages;
+	}
+	/* Set the p2m to the foreign pages. */
+	ret = i915_gem_foreign_set_p2m(fo);
+	if (ret) {
+		DRM_ERROR("Failed to set the p2m.\n");
+		goto fail_p2m;
+	}
+
+	ret = st_set_pages(&obj->mm.pages, fo->pvec, fo->num_pages);
+	if (ret < 0) {
+		DRM_ERROR("GEM foreign object failed: Not enough memory.\n");
+		goto fail_st;
+	}
+
+	ret = i915_gem_gtt_prepare_pages(obj, obj->mm.pages);
+	if (ret) {
+		DRM_ERROR("Failed to prepare DMA mapping for object.\n");
+		sg_free_table(obj->mm.pages);
+		obj->mm.pages = NULL;
+		goto fail_st;
+	}
+
+	sg_page_sizes = i915_sg_page_sizes(obj->mm.pages->sgl);
+
+	__i915_gem_object_set_pages(obj, obj->mm.pages, sg_page_sizes);
+
+	return 0;
+
+fail_st:
+	i915_gem_foreign_reset_p2m(fo);
+fail_p2m:
+	i915_gem_foreign_free_pages(fo);
+fail_pages:
+	kvfree(fo->pvec);
+
+	return -ENOMEM;
+}
+
+static void
+i915_gem_foreign_put_pages(struct drm_i915_gem_object *obj, struct sg_table *pages)
+{
+	if (!obj || !pages) return;
+	i915_gem_gtt_finish_pages(obj, pages);
+
+	if (obj->mm.pages)
+	{
+		sg_free_table(obj->mm.pages);
+		kfree(obj->mm.pages);
+	}
+	if (&obj->foreign)
+	{
+		i915_gem_foreign_reset_p2m(&obj->foreign);
+		i915_gem_foreign_free_pages(&obj->foreign);
+	}
+
+	if (obj->foreign.pvec)
+		kvfree(obj->foreign.pvec);
+}
+
+static void
+i915_gem_foreign_release(struct drm_i915_gem_object *obj)
+{
+	/* obj->foreign.pages should be free'd already through the put_page()
+	 * callback. */
+
+	if (likely(obj->foreign.mfns_ovr != NULL))
+		kfree(obj->foreign.mfns_ovr);
+	if (likely(obj->foreign.mfns != NULL))
+		kfree(obj->foreign.mfns);
+}
+
+static const struct drm_i915_gem_object_ops i915_gem_foreign_ops = {
+	.get_pages = i915_gem_foreign_get_pages,
+	.put_pages = i915_gem_foreign_put_pages,
+	.release = i915_gem_foreign_release,
+};
+
+/*
+ * Helper to initialise struct i915_gem_foreign embedded in struct drm_i915_gem_object.
+ * Fields in that struct require dynamic allocation depending on ioctl() input, those
+ * will be kfree()'d in the release() callback.
+ */
+static inline int
+i915_gem_foreign_init(struct drm_i915_gem_object *obj,
+		      uint64_t *mfns, size_t num_pages, uint32_t flags)
+{
+	struct i915_gem_foreign *fo = &obj->foreign;
+	int i, ret = 0;
+
+	fo->mfns_ovr = kcalloc(num_pages, sizeof (fo->mfns_ovr[0]), GFP_KERNEL);
+	if (!fo->mfns_ovr)
+		return -ENOMEM;
+
+	fo->mfns = kcalloc(num_pages, sizeof (fo->mfns[0]), GFP_KERNEL);
+	if (!fo->mfns) {
+		kfree(fo->mfns_ovr);
+		return -ENOMEM;
+	}
+	fo->num_pages = num_pages;
+	fo->flags = flags;
+
+	/* mfns are ok in an unsigned long, but userspace sends an array of uint64_t.
+	 * This is not a bottleneck, so we might as well re-arrange things to unsigned long
+	 * to keep the rest of the code sane. */
+	for (i = 0; i < num_pages; ++i)
+		fo->mfns[i] = (unsigned long)mfns[i];
+
+	return ret;
+}
+
+int
+i915_gem_foreign_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *file)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_foreign *args = data;
+	struct drm_i915_gem_object *obj;
+	size_t size = args->num_pages * PAGE_SIZE;
+	uint64_t *mfns;
+	int ret = 0;
+	u32 handle;
+
+	if (size > dev_priv->ggtt.vm.total)
+		return -E2BIG;
+
+	obj = i915_gem_object_alloc(dev_priv);
+	if (obj == NULL)
+		return -ENOMEM;
+
+	drm_gem_private_object_init(dev, &obj->base, size);
+	i915_gem_object_init(obj, &i915_gem_foreign_ops);
+	obj->cache_level = I915_CACHE_LLC;
+	obj->write_domain = I915_GEM_DOMAIN_CPU;
+	obj->read_domains = I915_GEM_DOMAIN_CPU;
+
+	mfns = kcalloc(args->num_pages, sizeof (mfns[0]), GFP_KERNEL);
+	if (mfns == NULL)
+		ret = -ENOMEM;
+	if (copy_from_user(mfns, args->mfns, args->num_pages * sizeof (mfns[0])))
+		ret = -EFAULT;
+
+	if (ret == 0)
+		ret = i915_gem_foreign_init(obj, mfns, args->num_pages,
+					    args->flags);
+	/* Release the mfns temporary array. */
+	kfree(mfns);
+
+	if (ret == 0)
+		ret = drm_gem_handle_create(file, &obj->base, &handle);
+	/*
+	 * Drop reference from allocate - handle holds it now.
+	 * If the previous call failed, this will drop the refcount to 0 and do
+	 * the cleanup in the release() callback.
+         */
+	i915_gem_object_put(obj);
+	if (ret)
+		return ret;
+
+	args->handle = handle;
+	return 0;
+}
--- a/drivers/gpu/drm/i915/i915_gem_object.h
+++ b/drivers/gpu/drm/i915/i915_gem_object.h
@@ -280,6 +280,14 @@ struct drm_i915_gem_object {
 
 	/** for phys allocated objects */
 	struct drm_dma_handle *phys_handle;
+	
+	struct i915_gem_foreign {
+		uint32_t flags;                 /* foreign region flags. */
+		unsigned long *mfns;            /* mfns "rent" from the guest */
+		size_t num_pages;               /* number of pages rented. */
+		unsigned long *mfns_ovr;        /* dom0 mfns overriden, used for reset. */
+		struct page **pvec;             /* vector of struct page backing the "rent" region. */
+	} foreign;
 
 	struct reservation_object __builtin_resv;
 };
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -319,6 +319,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_I915_PERF_ADD_CONFIG	0x37
 #define DRM_I915_PERF_REMOVE_CONFIG	0x38
 #define DRM_I915_QUERY			0x39
+#define DRM_I915_GEM_FOREIGN            0x50
 
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
 #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
@@ -377,6 +378,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_IOCTL_I915_PERF_ADD_CONFIG	DRM_IOW(DRM_COMMAND_BASE + DRM_I915_PERF_ADD_CONFIG, struct drm_i915_perf_oa_config)
 #define DRM_IOCTL_I915_PERF_REMOVE_CONFIG	DRM_IOW(DRM_COMMAND_BASE + DRM_I915_PERF_REMOVE_CONFIG, __u64)
 #define DRM_IOCTL_I915_QUERY			DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_QUERY, struct drm_i915_query)
+#define DRM_IOCTL_I915_GEM_FOREIGN		DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_FOREIGN, struct drm_i915_gem_foreign)
 
 /* Allow drivers to submit batchbuffers directly to hardware, relying
  * on the security mechanisms provided by hardware.
@@ -1429,6 +1431,14 @@ struct drm_i915_reset_stats {
 	__u32 pad;
 };
 
+struct drm_i915_gem_foreign {
+	__u64 __user *mfns;
+	__u32 num_pages;
+#define I915_FOREIGN_BALLOON_PAGES 0x00000001
+	__u32 flags;
+	__u32 handle;
+};
+
 struct drm_i915_gem_userptr {
 	__u64 user_ptr;
 	__u64 user_size;
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@ -2860,6 +2860,7 @@ static const struct drm_ioctl_desc i915_
 	DRM_IOCTL_DEF_DRV(I915_PERF_ADD_CONFIG, i915_perf_add_config_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_PERF_REMOVE_CONFIG, i915_perf_remove_config_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_QUERY, i915_query_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_GEM_FOREIGN, i915_gem_foreign_ioctl, DRM_RENDER_ALLOW),
 };
 
 static struct drm_driver driver = {
